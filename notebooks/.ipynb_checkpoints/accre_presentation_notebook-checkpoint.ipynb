{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import and Cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the CSV to use | instead of , to separate rows, then import [a selection] of the data as a dataframe\n",
    "for_pd = StringIO()\n",
    "with open('../data/accre-jobs-2020.csv') as accre:\n",
    "    for line in accre:\n",
    "        new_line = re.sub(r',', '|', line.rstrip(), count=12)\n",
    "        print (new_line, file=for_pd)\n",
    "for_pd.seek(0)\n",
    "accre_df = pd.read_csv(for_pd, sep='|')#[1000000:1005000] # add this to subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing NODES and CPUS to integers\n",
    "\n",
    "accre_df['NODES'] = accre_df['NODES'].astype(int)\n",
    "accre_df['CPUS'] = accre_df['CPUS'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting all times to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This splits the hour, minutes, seconds from the __TIME columns\n",
    "accre_df['hours_min_sec_req'] = accre_df['REQTIME'].str[-8:]\n",
    "accre_df['hours_min_sec_used'] = accre_df['USEDTIME'].str[-8:]\n",
    "\n",
    "## This splits the day from the ___TIME columns\n",
    "accre_df['day_req'] = accre_df['REQTIME'].str.extract('(.*?)-')\n",
    "accre_df['day_used'] = accre_df['USEDTIME'].str.extract('(.*?)-')\n",
    "\n",
    "## Adds zeros to the day column where null\n",
    "accre_df['day_req'] = accre_df['day_req'].fillna(0)\n",
    "accre_df['day_used'] = accre_df['day_used'].fillna(0)\n",
    "\n",
    "# Converting days to integers to use in converting to seconds\n",
    "accre_df['day_req'] = accre_df['day_req'].astype(int)\n",
    "accre_df['day_used'] = accre_df['day_used'].astype(int)\n",
    "\n",
    "# Converting to timedelta\n",
    "accre_df['hours_min_sec_req'] =  pd.to_timedelta(accre_df['hours_min_sec_req'], unit='s')\n",
    "accre_df['hours_min_sec_used'] =  pd.to_timedelta(accre_df['hours_min_sec_used'], unit='s')\n",
    "\n",
    "# Using timedeltas to then use dt.total_seconds()\n",
    "accre_df['hours_min_sec_req'] = accre_df['hours_min_sec_req'].dt.total_seconds()\n",
    "accre_df['hours_min_sec_used'] = accre_df['hours_min_sec_used'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total seconds columns \n",
    "accre_df['total_sec_req'] = (accre_df['day_req'] * 86400) + accre_df['hours_min_sec_req']\n",
    "accre_df['total_sec_used'] = (accre_df['day_used'] * 86400) + accre_df['hours_min_sec_used']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "While we have systematic checks in place to ensure the general system health of each compute node, we would like to use long-term data to see if there are any clusters of job failures on specific nodes. Do any of the production partition nodes show an unusual number of failed jobs relative to the others? Ignore the debug partition for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe subsetted to failed jobs and those found in the production partition\n",
    "\n",
    "accre_df_failures = accre_df[\n",
    "    (accre_df['STATE'] == 'FAILED') &\n",
    "    (accre_df['PARTITION'] == 'production')\n",
    "]\n",
    "\n",
    "accre_df_failures = accre_df_failures.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how much data we have now\n",
    "accre_df_failures.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with 3,816,290 in our dataset and are now down to 500 after whittling it down to failures in the production partition. This is a failure rate of .01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DF to plot by node using value_counts.  Will also rename the axis and reset the index\n",
    "failures_by_nodelist = accre_df_failures['NODELIST'].value_counts().rename_axis('NODELIST').reset_index(name='FAILURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_by_nodelist.plot(kind = 'hist', title = 'Failure Histogram', figsize = (10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_by_nodelist.plot(kind='box', color='red', title='Failures by Node');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_by_nodelist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node(s) that fail 3 or more times are considered outliers\n",
    "failures_by_nodelist[failures_by_nodelist['FAILURES'] > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be worth double checking these nodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the top failing nodes\n",
    "failures_by_nodelist.head(16).plot(\n",
    "    kind = 'bar', \n",
    "    x = 'NODELIST',\n",
    "    y = 'FAILURES',\n",
    "    title = 'Top 16 Failures by Node (Outliers)',\n",
    "    color = 'green', \n",
    "    figsize = (15,5),\n",
    "    rot = 25, \n",
    "    fontsize = 12.5\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After investigating the reason for failure by Exit Code, it was found that every failure was due to user error. It seems the systematic checks are performing as expected. That said, we want to see which accounts have many failures from user error as a potential opportunity for further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of failure by account\n",
    "accre_df_failures['ACCOUNT'].value_counts().plot(kind ='bar', title = 'Failure by Account', figsize=(10,5), rot=75);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF for failures by account and use box plot to identify outliers\n",
    "accre_df_failures_by_account = accre_df_failures['ACCOUNT'].value_counts()\n",
    "\n",
    "red_square = dict(markerfacecolor='r', marker='s')\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.title('Outliers from Failures By Account', fontsize=16)\n",
    "plt.xlabel('Failures')\n",
    "plt.annotate(s = 'cep', xy = (127, 1.05), fontsize = 12,\n",
    "             xytext = (126, 1.25), arrowprops=dict(facecolor='black', shrink=0.1, width=1))\n",
    "plt.annotate(s = 'plantain', xy = (88, .95), fontsize = 12,\n",
    "             xytext = (81, .75), arrowprops=dict(facecolor='black', shrink=0.1, width = 1))\n",
    "plt.annotate(s = 'tips', xy = (65, 1.05), fontsize = 12,\n",
    "             xytext = (65, 1.25), arrowprops=dict(facecolor='black', shrink=0.1, width = 1))\n",
    "plt.boxplot(accre_df_failures_by_account,  \n",
    "            vert = False, \n",
    "            flierprops=red_square);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accounts cep, plantain, and tips are failing the most due to user error. I'd suggest reaching out to them to see if they need additional help when running jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "The CMS collaboration has an automated job submission system that runs jobs as \"cmslocal\" and \"cmspilot\". For these two users, jobs have internal system tests that will terminate their jobs early after approximately 30 minutes. Do any of their jobs that ended in under an hour also cluster on specific compute nodes, suggesting possbily unreliable systems? Check both “production” and “nogpfs” partitions. Look for commonly failing nodes and compare with other failed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting data for cms account\n",
    "cms_df = accre_df[accre_df['ACCOUNT']=='cms']\n",
    "\n",
    "#subsetting data to get rid of debug partition\n",
    "cms_df = cms_df[cms_df['PARTITION']!='debug']\n",
    "\n",
    "#subsetting data for only 'cmspilot' and 'cmslocal' users\n",
    "cms_df = cms_df[(cms_df['USER']=='cmspilot')|(cms_df['USER']=='cmslocal')]\n",
    "\n",
    "#printing how many CMS jobs including over an hour\n",
    "print(cms_df.shape)\n",
    "\n",
    "#subsetting for time under an hour\n",
    "cms_df = cms_df[cms_df['total_sec_used'] <= 3600]\n",
    "\n",
    "#getting only failed jobs\n",
    "#using state != to complete instead of exit code beacuse its built in their code to crash and error might not be reflected in exit code\n",
    "cms_failed = cms_df[cms_df['STATE'] != 'COMPLETED']\n",
    "cms_completed = cms_df[cms_df['STATE'] == 'COMPLETED']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining the difference between a 'COMPLETED' job and exitcode '0:0'\n",
    "pd.crosstab(cms_df['EXITCODE'],cms_df['STATE']).apply(lambda x: (x/x.sum()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at distribution of seconds to find spike\n",
    "#where jobs are canceling sround 30 min\n",
    "cms_df.hist(column = 'total_sec_used');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many jobs ended between 500 and 2000 seconds\n",
    "#or 1000 and 1500 use this one second spike\n",
    "cms_jobs_ended_around_30_min = cms_df[(cms_df['total_sec_used'] < 1500) & (cms_df['total_sec_used'] > 1000)]\n",
    "\n",
    "print(\"Number of CMS Jobs Total:\")\n",
    "print(\"(699831, 19)\")\n",
    "print(\"Number of CMS Jobs Under Hour:\")\n",
    "print(cms_df.shape)\n",
    "print(\"Number of CMS Jobs Ended Around 30 Minutes:\")\n",
    "print(cms_jobs_ended_around_30_min.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what percent of jobs is this?\n",
    "print(str(round((263645/699831)*100,2)) + \"% are ending around 30 minutes\\nout of total CMS jobs (cmspilot/cmslocal users)\")\n",
    "print(\"\\n\")\n",
    "print(str(round((263645/447255)*100,2)) + \"% are ending around 30 minutes\\nout of jobs ending in less than an hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most failed nodes for CMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_failed_nodes = cms_df['NODELIST'].value_counts().head(20)\n",
    "ax = most_failed_nodes.plot(kind = 'bar', figsize = (8,6))\n",
    "ax.set_title('CMS Account: Nodes that Failed the Most',weight='bold', size='large')\n",
    "ax.set_xlabel('Node')\n",
    "ax.set_ylabel('Number of Time Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of failed nodes for new df for percents\n",
    "cms_failed['NODELIST'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of successful nodes for new df for percents\n",
    "cms_completed['NODELIST'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating df with top nodes to examine by percent of times failed\n",
    "CMS_nodes = {'NODE':['ng518','cn1314','cn394','ng734','cn475','cn1094','ng1112','cn449','cn1121','cn304','cn1394','cn408','cn1387','cn399','cn363','cn429','cn1398','cn312'],\n",
    "            'TIMES_FAILED':[16,12,11,10,9,9,9,9,9,9,8,8,8,8,8,8,8,8],\n",
    "            'TOTAL_TIMES_USED':[19352,12,347,4138,364,397,6171,400,465,298,408,446,249,415,339,306,550,407]}\n",
    "CMS_nodes_df = pd.DataFrame(CMS_nodes, columns = ['NODE','TIMES_FAILED','TOTAL_TIMES_USED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_nodes_df['PERCENT_FAILED'] = (CMS_nodes_df['TIMES_FAILED']/CMS_nodes_df['TOTAL_TIMES_USED'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotted with one outlier, maybe remove it to make it easier to see\n",
    "ax = CMS_nodes_df.sort_values('PERCENT_FAILED').plot.bar(x = 'NODE', y = 'PERCENT_FAILED', figsize = (8,6))\n",
    "ax.set_title('Percent Node Failed for CMS Account',weight='bold', size='large')\n",
    "ax.set_xlabel('Node')\n",
    "ax.set_ylabel('Percent of Times Failed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed cn 1314 to make it to make it easier to see,  label this graph\n",
    "CMS_nodes_without_cn1314 = CMS_nodes_df[CMS_nodes_df['NODE'] != 'cn1314']\n",
    "ax = CMS_nodes_without_cn1314.sort_values('PERCENT_FAILED').plot.bar(x = 'NODE', y = 'PERCENT_FAILED',figsize = (8,6))\n",
    "ax.set_title('Percent Node Failed for CMS Account (without Outlier CN1314)',weight='bold', size='large')\n",
    "ax.set_xlabel('Node')\n",
    "ax.set_ylabel('Percent of Times Failed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_nodes_df.sort_values('PERCENT_FAILED', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "What groups are best optimizing their memory usage in terms of percent of actual memory used of the memory requested for a job? What is the average percent for each group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DF subset to exit code 0:0 and production partition\n",
    "accre_df_mem = accre_df\n",
    "accre_df_mem = accre_df_mem[\n",
    "    (accre_df['EXITCODE'] == '0:0') &\n",
    "    (accre_df['PARTITION'] == 'production')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns, set axis = 1 to specify dropping columns\n",
    "accre_df_mem = accre_df_mem.drop(['JOBID','USER', 'REQTIME', 'USEDTIME', 'hours_min_sec_req', 'hours_min_sec_used','day_req', 'day_used', 'NODELIST'], axis =1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column is for requested memory per node\n",
    "accre_df_mem['RMPN'] = accre_df_mem['REQMEM'].str.extract('(.*)Mn$')\n",
    "\n",
    "#new column is for requested memory per core\n",
    "accre_df_mem['RMPC'] = accre_df_mem['REQMEM'].str.extract('(.*)Mc$')\n",
    "\n",
    "#new column is for requested memory per core\n",
    "accre_df_mem['UM'] = accre_df_mem['USEDMEM'].str.extract('(.*)M$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change RMPC,RMPN,UM columns to fill NaNs with a 0\n",
    "accre_df_mem['RMPC'] = accre_df_mem['RMPC'].fillna('0')\n",
    "accre_df_mem['RMPN'] = accre_df_mem['RMPN'].fillna('0')\n",
    "accre_df_mem['UM'] = accre_df_mem['UM'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing types\n",
    "accre_df_mem['RMPC']= accre_df_mem['RMPC'].astype(str).astype(float)\n",
    "accre_df_mem['RMPN']= accre_df_mem['RMPN'].astype(str).astype(float)\n",
    "accre_df_mem['UM']= accre_df_mem['UM'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is per core??? assume you multiply cpus by node this column will be RMPN times CPUS \n",
    "#3.5 is the average node to core ratio\n",
    "accre_df_mem['RMPN'] = accre_df_mem['RMPN'] /(accre_df_mem['CPUS']/ accre_df_mem['NODES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is per core??? update RMPC to add the RMPNEW column\n",
    "accre_df_mem['RMPC'] = accre_df_mem['RMPC'] + accre_df_mem['RMPN']\n",
    "\n",
    "# remove unneeded columns\n",
    "accre_df_mem = accre_df_mem.drop([\"RMPN\"], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert used memory to cores \n",
    "accre_df_mem['UM'] = (accre_df_mem['UM'] /accre_df_mem['CPUS'])/ accre_df_mem['NODES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCENT of request to actual used\n",
    "accre_df_mem['PRU'] = (accre_df_mem['UM']/accre_df_mem['RMPC']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accre_df_mem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accre_df_mem.groupby('ACCOUNT')['PRU'].agg('mean').sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Optimizing memory is more important for longer running jobs then shorter running jobs as the resources are tied up for longer. If jobs are weighted by runtime, what is the average percent of memory used of the requested memory for each group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavg(group, avg_name, weight_name):\n",
    "    \"\"\" http://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns\n",
    "    In rare instance, we may not have weights, so just return the mean. Customize this if your business case\n",
    "    should return otherwise.\n",
    "    \"\"\"\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavg(accre_df_mem, \"PRU\", \"total_sec_used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
